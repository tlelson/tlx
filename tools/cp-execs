#!/usr/bin/env python

import argparse
from concurrent.futures import ThreadPoolExecutor as TPE
import asyncio
from typing import Generator, Dict
import boto3
from botocore import config
import json
import logging
from datetime import datetime

# python -m pip install -U 'boto3-stubs[essential]'
# python -m pip install -U 'boto3-stubs[codepipeline]'
from mypy_boto3_codepipeline.type_defs import (
    ListPipelineExecutionsOutputTypeDef,
    PipelineExecutionSummaryTypeDef as PES,
    ActionExecutionFilterTypeDef as AEF,
    ActionExecutionDetailTypeDef as ExecutionDetail,
)
from mypy_boto3_codepipeline.client import CodePipelineClient

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger()

session = boto3.Session()

def _json_default(obj):
    if isinstance(obj, datetime):
        return obj.isoformat(timespec='minutes')
    return str(obj)

def raw_executions(cp: CodePipelineClient, pipeline_name: str, count) -> Generator[PES, None, None]:
    """ Returns latest `count` executions.
    """
    i = 0
    more = True
    lpe_args = {"pipelineName": pipeline_name, "maxResults": 10}
    while more and (i<count):
        res: ListPipelineExecutionsOutputTypeDef = cp.list_pipeline_executions(**lpe_args)
        for execution in res["pipelineExecutionSummaries"]:
            i +=1
            yield execution
        more = res.get("nextToken", False)
        lpe_args["nextToken"] = res.get("nextToken")


def execution_details(cp: CodePipelineClient, pipeline_name: str, execution_id: str) -> list[ExecutionDetail]:
    filter: AEF = {"pipelineExecutionId": execution_id}
    res = cp.list_action_executions(
        pipelineName=pipeline_name, filter=filter)

    return res["actionExecutionDetails"]


class ExecutionSummary:
    def __init__(self, pipeline_name, count=10, concurrency=10) -> None:
        self._pipeline_name = pipeline_name
        self._count = count
        self._executor = TPE(max_workers=concurrency)
        self._cp: CodePipelineClient = session.client('codepipeline', config=config.Config(
            max_pool_connections=concurrency,
        ))

    def run(self) -> list[dict]:
        """Returns a list of Execution summararies.  Return type varies on the number and names of
        the 'Source' that are configured to each pipeline."""
        return asyncio.run(self._compile())

    async def _compile(self) -> list[dict]:

        tasks = []
        for pes in raw_executions(self._cp, self._pipeline_name, self._count):
            task = self._build_execution(pes)
            tasks.append(task)

        # These are ordered, so the should be printed in order
        return await asyncio.gather(*tasks)

    async def _build_execution(self, pes: PES) -> dict:
        e = {
            "StartTime": "",
            "Ver": "",
            "StagesRan": 0,
            "FinalStage": "",
            "Status": pes.get("status"),
        }
        for source in pes.get("sourceRevisions", []):
            e[source["actionName"]] = source.get("revisionId", "")[:8]

        # looks syncronous but needed to allow other threads running the same command for a
        # different execution.
        loop = asyncio.get_event_loop()
        ed_task = loop.run_in_executor(self._executor,
            execution_details, self._cp, self._pipeline_name,
            pes.get("pipelineExecutionId", ""))

        eds = await ed_task
        e["StagesRan"] = len(set(ed.get("stageName") for ed in eds))
        last_stage = eds[0]
        del eds  # only need the last_stage now

        e["Ver"] = last_stage.get("pipelineVersion")
        e["FinalStage"] = last_stage.get("stageName")
        if st := last_stage.get("startTime"):
            e["StartTime"] = st

        e["ExecutionId"] = pes.get("pipelineExecutionId")
        return e

if __name__ == "__main__":
    # Initialize the parser
    parser = argparse.ArgumentParser(
        description="List Codepipeline past executions in reverse cronological order")
    parser.add_argument('pipeline_name', type=str, help='Name of the pipeline.')
    parser.add_argument('count', type=int, nargs='?', default=10,
                        help='Optional count parameter (default is 10).')
    # Parse the arguments
    args = parser.parse_args()

    e = ExecutionSummary(args.pipeline_name, args.count)
    executions = e.run()
    print(json.dumps(executions, default=_json_default, sort_keys=False))
